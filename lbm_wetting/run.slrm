#!/usr/bin/zsh

### Task name
#SBATCH --account=rwth1711
#SBATCH --job-name=two-phase_lbm

### Output file
#SBATCH --output=/hpcwork/fw641779/lbm/output_logs/two-phase_lbm_%j.out

### Ask for less than 4 GB memory per (CPU) task = MPI rank
#SBATCH --mem-per-cpu=2000

### Start a parallel job for a distributed-memory system on several nodes
#SBATCH --nodes=1

### Number of tasks (MPI ranks)
#SBATCH --ntasks=48

### Mail notification configuration
#SBATCH --mail-type=ALL
#SBATCH --mail-user=florian.wiesner@avt.rwth-aachen.de

### create time series, i.e. 100 jobs one after another. Each runs for 24 hours
##SBATCH --array=1-100%1

### Maximum runtime per task
#SBATCH --time=24:00:00

### use nodes exclusively, only needed for larger simulations
#SBATCH --exclusive

module purge
module load GCC/11.3.0
module load OpenMPI/4.1.4

palabos_dir="/home/fw641779/Coding/lattice-boltzmann-wetting/mplbm-ut-mirror/src/2-phase_LBM/ShanChen"

config_file="/home/fw641779/Coding/lattice-boltzmann-wetting/lbm_wetting/twophase.yml"
sim_dir="/hpcwork/fw641779/lbm/Test"
sim_name="test_run_2"

#conda activate lbm
python3 /home/fw641779/Coding/lattice-boltzmann-wetting/lbm_wetting/2_phase_sim.py $config_file $sim_dir $sim_name
return_code=$?
if [ $return_code -ne 0 ]; then
    echo "Python script failed with code $return_code. Exiting..."
    exit $return_code
else
    # Else continue
    input_file=$sim_dir/$sim_name/input/2_phase_sim_input.xml
    $MPIEXEC $FLAGS_MPI_BATCH $palabos_dir $input_file
fi
